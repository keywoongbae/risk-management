?norm
plotdist(samples[, 1])
plotdist(samples_A)
samples
samples[,1]
plotdist(samples[,1])
plotdist(samples_A)
# 음이항 분포 파라미터
size <- 5
prob <- 0.5
# 생존 확률 계산 함수 (음이항 + 경험적 심도)
survival_probability_negbinom <- function(threshold) {
negbinom_frequency <- rnbinom(1000, size, prob)
losses <- sapply(negbinom_frequency, function(n) sum(sample(severity_data, n, replace = TRUE)))
mean(losses < threshold)
}
# 예시 생존 확률 계산
sp_negbinom <- survival_probability_negbinom(threshold)
# ----------------------------------------------------------------------------------
# 필요한 패키지 로드
library(fitdistrplus)
library(boot)
# 예제 데이터 생성 (경험적 심도 데이터)
severity_data <- rexp(100, rate = 1/500)  # 평균 500의 지수 분포
# 포아송 빈도 파라미터
lambda <- 10
# 부트스트랩 함수 정의
bootstrap_function <- function(data, indices) {
resampled_data <- data[indices]
mean(resampled_data)
}
# 부트스트랩 적용
bootstrap_results <- boot(severity_data, bootstrap_function, R = 1000)
bootstrap_ci <- boot.ci(bootstrap_results, type = "perc")
# 생존 확률 계산
survival_probability <- function(threshold) {
poisson_frequency <- rpois(1000, lambda)
losses <- sapply(poisson_frequency, function(n) sum(sample(severity_data, n, replace = TRUE)))
mean(losses < threshold)
}
# 예시 생존 확률 계산
threshold <- 10000
sp <- survival_probability(threshold)
# 결과 출력
cat("Survival Probability:", sp, "\n")
cat("Bootstrap Confidence Interval:", bootstrap_ci$percent[4:5], "\n")
# 음이항 분포 파라미터
size <- 5
prob <- 0.5
# 생존 확률 계산 함수 (음이항 + 경험적 심도)
survival_probability_negbinom <- function(threshold) {
negbinom_frequency <- rnbinom(1000, size, prob)
losses <- sapply(negbinom_frequency, function(n) sum(sample(severity_data, n, replace = TRUE)))
mean(losses < threshold)
}
# 예시 생존 확률 계산
sp_negbinom <- survival_probability_negbinom(threshold)
# 결과 출력
cat("Survival Probability with Negative Binomial Frequency:", sp_negbinom, "\n")
# 필요한 패키지 로드
library(copula)
# 클레이튼 코퓰라 설정
theta <- 2  # 상관계수 0.2에 대응하는 클레이튼 코퓰라 파라미터
clayton_copula <- claytonCopula(param = theta, dim = 2)
# 생존 확률 계산 함수 (클레이튼 코퓰라 + 경험적 심도)
survival_probability_clayton <- function(threshold) {
copula_samples <- rCopula(1000, clayton_copula)
poisson_frequency <- qpois(copula_samples[, 1], lambda)
losses <- sapply(poisson_frequency, function(n) sum(sample(severity_data, n, replace = TRUE)))
mean(losses < threshold)
}
# 예시 생존 확률 계산
sp_clayton <- survival_probability_clayton(threshold)
# 결과 출력
cat("Survival Probability with Clayton Copula:", sp_clayton, "\n")
# ============================================================
# Clayton copula
# ============================================================
library("copula")
clayton_object <- claytonCopula(dim=3)
clayton_object
# ============================================================
# Clayton copula
# ============================================================
final.freq$num
# ============================================================
# Clayton copula
# ============================================================
plotdist(final.freq$num)
X <- final.freq$num
Y <- sas$Loss.Amount...M.
clayton_object <- claytonCopula(dim=2)
clayton_object
fitted_clayton_ml <- fitCopula(clayton_object, c(X,Y), method="ml")
qnbinom
?qnbinom
q <- qnbinom(copula_samples[,1]
, size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )
clayton_object <- claytonCopula(dim=2)
# with dependency parameter corresponding to a correlation of 0.2
# tau is 0.2, so theta is 0.5
clayton_object
copula_samples <- rCopula(1000, clayton_object)
q <- qnbinom(copula_samples[,1]
, size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )
# with dependency parameter corresponding to a correlation of 0.2
# tau is 0.2, so theta is 0.5
clayton_object
copula_samples <- rCopula(1000, clayton_object)
clayton_object <- claytonCopula(alpha = 0.5, dim=2)
clayton_object <- claytonCopula(theta = 0.5, dim=2)
?claytonCopula
clayton_object <- claytonCopula(parma = 0.5, dim=2)
clayton_object <- claytonCopula(param = 0.5, dim=2)
# with dependency parameter corresponding to a correlation of 0.2
# tau is 0.2, so theta is 0.5
clayton_object
copula_samples <- rCopula(1000, clayton_object)
q <- qnbinom(copula_samples[,1]
, size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )
losses <- sapply(q, function(n) sum(sample(Y, n, replace = TRUE)))
losses
plotdist(losses)
# ============================================================
# Clayton copula
# ============================================================
num_simulations <- 50
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom_est$estimate["size"], mu = nbinom_est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd_model$fitted.values["scale"],
xi = gpd_model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd_model$fitted.values["scale"],
xi = gpd_model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
for (i in 1:num_simulations){
N <- rnbinom(1
,size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )+1
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
}
library("rlang")
library("ggplot2")
library("MASS") # import plotdist
library("fitdistrplus") #fitdist
library("POT") # import fitgpd
library("quantmod")
# ============================================================
# Import libraries and SAS OpRisk Global dataset
# ============================================================
install.packages("quantmod")
library("quantmod")
library("lubridate")
library("evmix")
library("psych")
# ============================================================
# Clayton copula
# ============================================================
num_simulations <- 50
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd_model$fitted.values["scale"],
xi = gpd_model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
### referred by EC(2015) annex II
s.general_liability_p <- 0.14
s.general_liability_r <- 0.11
s.legal_expenses_p <- 0.07
s.legal_expenses_r <- 0.12
s.mis_financial_p <- 0.13
s.mis_financial_r <- 0.20
### referred by EC (2015) annex XI and XII
s.liability <- 1/3
s.other_cat_risk <- 0.4/3
corr.cat <- 0
### calculate s of 3 LoBs
s.general_liability <- round(sqrt((s.general_liability_p/2)^2+(s.general_liability_r/2)^2+2*0.5*(s.general_liability_p/2)*(s.general_liability_r/2)),2)
s.legal_expenses <- round(sqrt((s.legal_expenses_p/2)^2+(s.legal_expenses_r/2)^2+2*0.5*(s.legal_expenses_p/2)*(s.legal_expenses_r/2)),2)
s.mis_financial <- round(sqrt((s.mis_financial_p/2)^2+(s.mis_financial_r/2)^2+2*0.5*(s.mis_financial_p/2)*(s.mis_financial_r/2)),2)
### calculate s of premium and reserve risk
s.premium_reserve <- round(sqrt((s.general_liability/3)^2
+(s.legal_expenses/3)^2+(s.mis_financial/3)^2
+(s.general_liability/3)*((s.legal_expenses/3)+(s.mis_financial/3))
+(s.legal_expenses/3)*(s.mis_financial/3)),2)
s.premium_reserve
### calculate s of catastrophe risk
s.catastrophe <- round(sqrt((s.liability/2)^2 + (s.other_cat_risk/2)^2),2)
s.catastrophe
### calculate s of non-life module
corr.non_life <- 0.25
s.non_life <- round(sqrt(s.premium_reserve^2
+ (s.catastrophe/2)^2
+ 2*corr.non_life
*s.premium_reserve
*(s.catastrophe/2)),2)
s.non_life
### (1)-1 estimate the frequency distribution
sas$date_year <- as.Date(paste0(format(as.Date(sas$Month...Year.of.Settlement)
, "%Y-%m"), "-01"))
freq.data <- data.frame(table(sas$date_year))
colnames(freq.data) <- c("date", "num")
freq.data$date <- as.Date(freq.data$date)
freq.data
all.dates <- seq(min(as.Date(sas$date_year)), max(as.Date(sas$date_year))
, by = "1 month")
missing.dates <- as.Date((setdiff(as.Date(all.dates), as.Date(sas$date_month))))
missing.dates.df <- data.frame(date_column = missing.dates, value = 0)
colnames(missing.dates.df) <- c("date", "num")
missing.dates.df$num <- as.integer(missing.dates.df$num)
combined.freq <- rbind(freq.data, missing.dates.df)
final.freq <- combined.freq[order(combined.freq$date), ]
plotdist(final.freq$num)
nbinom.est <- fitdist(final.freq$num, "nbinom")
nbinom.est
### (1)-2 estimate the severity distribution
plotdist(log(sas$Loss.Amount...M.))
threshold <- 2
loss.body <- sas$Loss.Amount...M.[sas$Loss.Amount...M.<threshold]
loss.tail <- sas$Loss.Amount...M.[sas$Loss.Amount...M.>= threshold]
lnorm_model <- fitdist(loss.body, "lnorm", method = "mle")
gpd.model <- fitgpd(loss.tail, 0, "mle")
lnorm_model
gpd.model
# (2) truncate the severity distribution
Z <- pmin(Z, 50)
mean(Z)
sd
plotdist(Z)
### (3) LDA with MCMC simulation
num_simulations <- 50
total_losses <- numeric(num_simulations)
for (i in 1:num_simulations){
N <- rnbinom(1
,size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )+1
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
}
X <- total_losses
plotdist(X)
# (5) summary
mean(Z) # in paper, 43.9
sd(Z) # in paper, 429.9
mean(log(Z)) # in paper, mean(ln(Z)) is 0.8
sd(log(Z)) # in paper, std(ln(Z)) is 2.1
mean(X) # in paper, E[X] is 3.2
sd(X) # in paper, std(X) is 116.4
quantile(X, 0.995)
mean(X[X > quantile(X, 0.99)])
# ============================================================
# Clayton copula
# ============================================================
num_simulations <- 50
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd_model$fitted.values["scale"],
xi = gpd_model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
s# 생존 확률 계산
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
s# 생존 확률 계산
# 생존 확률 계산
survival_probability <- mean(total_losses < threshold)
# 결과 출력
cat("Survival Probability with Clayton Copula (correlation 0.2):", survival_probability, "\n")
total_losses
threshold
source("C:/Users/Keywoong/OneDrive - postech.ac.kr/깃허브 모음/risk-management-papers/code/Eling and Schnell (2019).r", echo=TRUE)
### referred by EC(2015) annex II
s.general_liability_p <- 0.14
s.general_liability_r <- 0.11
s.legal_expenses_p <- 0.07
s.legal_expenses_r <- 0.12
s.mis_financial_p <- 0.13
s.mis_financial_r <- 0.20
### referred by EC (2015) annex XI and XII
s.liability <- 1/3
s.other_cat_risk <- 0.4/3
corr.cat <- 0
### calculate s of 3 LoBs
s.general_liability <- round(sqrt((s.general_liability_p/2)^2+(s.general_liability_r/2)^2+2*0.5*(s.general_liability_p/2)*(s.general_liability_r/2)),2)
s.legal_expenses <- round(sqrt((s.legal_expenses_p/2)^2+(s.legal_expenses_r/2)^2+2*0.5*(s.legal_expenses_p/2)*(s.legal_expenses_r/2)),2)
s.mis_financial <- round(sqrt((s.mis_financial_p/2)^2+(s.mis_financial_r/2)^2+2*0.5*(s.mis_financial_p/2)*(s.mis_financial_r/2)),2)
### calculate s of premium and reserve risk
s.premium_reserve <- round(sqrt((s.general_liability/3)^2
+(s.legal_expenses/3)^2+(s.mis_financial/3)^2
+(s.general_liability/3)*((s.legal_expenses/3)+(s.mis_financial/3))
+(s.legal_expenses/3)*(s.mis_financial/3)),2)
s.premium_reserve
### calculate s of catastrophe risk
s.catastrophe <- round(sqrt((s.liability/2)^2 + (s.other_cat_risk/2)^2),2)
s.catastrophe
### calculate s of non-life module
corr.non_life <- 0.25
s.non_life <- round(sqrt(s.premium_reserve^2
+ (s.catastrophe/2)^2
+ 2*corr.non_life
*s.premium_reserve
*(s.catastrophe/2)),2)
s.non_life
### (1)-1 estimate the frequency distribution
sas$date_year <- as.Date(paste0(format(as.Date(sas$Month...Year.of.Settlement)
, "%Y-%m"), "-01"))
freq.data <- data.frame(table(sas$date_year))
colnames(freq.data) <- c("date", "num")
freq.data$date <- as.Date(freq.data$date)
freq.data
all.dates <- seq(min(as.Date(sas$date_year)), max(as.Date(sas$date_year))
, by = "1 month")
missing.dates <- as.Date((setdiff(as.Date(all.dates), as.Date(sas$date_month))))
missing.dates.df <- data.frame(date_column = missing.dates, value = 0)
colnames(missing.dates.df) <- c("date", "num")
missing.dates.df$num <- as.integer(missing.dates.df$num)
combined.freq <- rbind(freq.data, missing.dates.df)
final.freq <- combined.freq[order(combined.freq$date), ]
plotdist(final.freq$num)
nbinom.est <- fitdist(final.freq$num, "nbinom")
nbinom.est
### (1)-2 estimate the severity distribution
plotdist(log(sas$Loss.Amount...M.))
threshold <- 2
loss.body <- sas$Loss.Amount...M.[sas$Loss.Amount...M.<threshold]
loss.tail <- sas$Loss.Amount...M.[sas$Loss.Amount...M.>= threshold]
lnorm_model <- fitdist(loss.body, "lnorm", method = "mle")
gpd.model <- fitgpd(loss.tail, 0, "mle")
lnorm_model
gpd.model
# (2) truncate the severity distribution
Z <- pmin(Z, 50)
mean(Z)
sd
plotdist(Z)
### (3) LDA with MCMC simulation
num_simulations <- 50
total_losses <- numeric(num_simulations)
for (i in 1:num_simulations){
N <- rnbinom(1
,size=nbinom.est$estimate["size"]
, mu=nbinom.est$estimate["mu"] )+1
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
}
X <- total_losses
plotdist(X)
# (5) summary
mean(Z) # in paper, 43.9
sd(Z) # in paper, 429.9
mean(log(Z)) # in paper, mean(ln(Z)) is 0.8
sd(log(Z)) # in paper, std(ln(Z)) is 2.1
mean(X) # in paper, E[X] is 3.2
sd(X) # in paper, std(X) is 116.4
quantile(X, 0.995)
mean(X[X > quantile(X, 0.99)])
# ============================================================
# Clayton copula
# ============================================================
num_simulations <- 50
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
# ============================================================
# Clayton copula
# ============================================================
library(copula)
clayton_object <- claytonCopula(dim=3)
theta <- 0.5  # 상관계수 0.2에 대응하는 클레이튼 코퓰라 파라미터
clayton_copula <- claytonCopula(param = theta, dim = 2)
num_simulations <- 50
for (i in 1:num_simulations) {
# 코퓰라 샘플 생성
copula_samples <- rCopula(1, clayton_copula)
# 빈도 샘플 생성 (음이항 분포)
N <- qnbinom(copula_samples[, 1], size = nbinom.est$estimate["size"], mu = nbinom.est$estimate["mu"]) + 1
# 심도 샘플 생성 (로그-정규 + GPD)
if (N > 0) {
Z <- rlognormgpd(N,
lnmean = lnorm_model$estimate["meanlog"],
lnsd = lnorm_model$estimate["sdlog"],
u = 2,
sigmau = gpd.model$fitted.values["scale"],
xi = gpd.model$fitted.values["shape"])
total_losses[i] <- sum(Z)
} else {
total_losses[i] <- 0
}
}
total_losses
plotdist(total_losses)
library("rlang")
library("ggplot2")
library("MASS") # import plotdist
library("fitdistrplus") #fitdist
library("POT") # import fitgpd
library("quantmod")
library("lubridate")
library("evmix")
library("psych")
getwd()
sas <- read.csv("../../dataset/SAS.csv")
sas$Month...Year.of.Settlement <- ymd(sas$Month...Year.of.Settlement)
sas <- subset(sas, Month...Year.of.Settlement < ymd("2015-01-01"))
sas <- subset(sas, Month...Year.of.Settlement > ymd("1995-01-01"))
sas <- sas[sas$cyber_risk==1,]
length(sas$Reference.ID.Code)
length(sas$Loss.Amount...M.) # Length is 1593
describe(sas$Loss.Amount...M.)
summary(sas$Loss.Amount...M.)
